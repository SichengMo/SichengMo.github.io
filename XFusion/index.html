<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta content="X-Fusion: Introducing New Modality to Frozen Large Language Models" name="description">
  <meta content="multimodal chatbot" name="keywords">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="./static/images/xfusion-teaser-2.gif" property="og:image" />
  <meta content="X-Fusion: Introducing New Modality to Frozen Large Language Models" property="og:title" />
  <meta content="X-Fusion: Introducing New Modality to Frozen Large Language Models" property="og:description" />
  <link href="jemdoc.css" rel="stylesheet" type="text/css" />
  <title>X-Fusion</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css" rel="stylesheet">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" rel="stylesheet">
  <link href="./static/css/index.css" rel="stylesheet">
  <link href="./static/images/logo.png" rel="icon">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js" type="module"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PJGLWPJ36R"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PJGLWPJ36R');
  </script>
</head>

<style>
    .expandable-card .card-text-container {
        max-height: 200px;
        overflow-y: hidden;
        position: relative;
    }

    .expandable-card.expanded .card-text-container {
        max-height: none;
    }

    .center {
        margin-left: auto;
        margin-right: auto;
    }

    .expand-btn {
        position: relative;
        display: none;
        background-color: rgba(255, 255, 255, 0.8);
        /* margin-top: -20px; */
        /* justify-content: center; */
        color: #510c75;
        border-color: transparent;
    }

    .expand-btn:hover {
        background-color: rgba(200, 200, 200, 0.8);
        text-decoration: none;
        border-color: transparent;
        color: #510c75;
    }

    .expand-btn:focus {
        outline: none;
        text-decoration: none;
    }

    .expandable-card:not(.expanded) .card-text-container:after {
        content: "";
        position: absolute;
        bottom: 0;
        left: 0;
        width: 100%;
        height: 90px;
        background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
    }

    .expandable-card:not(.expanded) .expand-btn {
        margin-top: -40px;
    }

    .card-body {
        padding-bottom: 5px;
    }

    .vertical-flex-layout {
        justify-content: center;
        align-items: center;
        height: 100%;
        display: flex;
        flex-direction: column;
        gap: 5px;
    }

    .figure-img {
        max-width: 100%;
        height: auto;
    }

    .adjustable-font-size {
        font-size: calc(0.5rem + 2vw);
    }

    .chat-history {
        flex-grow: 1;
        overflow-y: auto;
        /* overflow-x: hidden; */
        padding: 5px;
        border-bottom: 1px solid #ccc;
        margin-bottom: 10px;
    }

    #gradio pre {
        background-color: transparent;
    }
</style>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="color:#06402b;">
            <img src='./static/images/xfusion.gif' style="height: 60px"></img>
          </h1>
          <h3 class="title is-2 publication-title">Introducing New Modality to Frozen Large Language Models</h3>
          <div class="is-size-5 publication-authors">
              <span class="author-block">
                <br>
                <a href="https://sichengmo.github.io/" style="color:#f68946;font-weight:normal;">Sicheng Mo<sup>1</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>
            <span class="author-block">
                <!-- <br> -->
                <a href="https://thaoshibe.github.io/" style="color:#f68946;font-weight:normal;">Thao Nguyen<sup>2</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>

            <span class="author-block">
                <a href="https://www.xunhuang.me/" style="color:#f68946;font-weight:normal;">Xun Huang<sup>3</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>

            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7uTobWwAAAAJ&hl=en"
                   style="color:#f68946;font-weight:normal;">Siddharth Srinivasan Iyer<sup>3</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>

            <span class="author-block">
                <a href="https://yijunmaverick.github.io/"
                   style="color:#f68946;font-weight:normal;">Yijun Li<sup>3</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>

            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Pl5uJlUAAAAJ&hl=en"
                   style="color:#f68946;font-weight:normal;">Yucheng Liu<sup>3</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>

            <span class="author-block">
                <a href="https://tandon-a.github.io/"
                   style="color:#f68946;font-weight:normal;">Abhishek Tandon<sup>3</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>

            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=B_FTboQAAAAJ&hl=en"
                   style="color:#f68946;font-weight:normal;">Eli Shechtman<sup>3</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>
                        <span class="author-block">
                <a href="https://krsingh.cs.ucdavis.edu/"
                   style="color:#f68946;font-weight:normal;">Krishna Kumar Singh<sup>3</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>

            <span class="author-block">
                <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae
                  Lee<sup>2</sup></a>
                  &nbsp;&nbsp;&nbsp;
              </span>

            <span class="author-block">
                <a href="https://boleizhou.github.io/" style="color:#f68946;font-weight:normal;">Bolei Zhou<sup>1</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>

            <span class="author-block">
                <a href="https://yuheng-li.github.io/"
                   style="color:#F2A900;font-weight:normal;">Yuheng Li<sup>3</sup></a>
              <!-- &nbsp;&nbsp;&nbsp; -->
              </span>
          </div>
          <div class="is-size-5 publication-authors">
            <div>
              <a href="https://www.cs.wisc.edu/" style="margin-right: 30px;">
                <img src="./static/images/ucla-logo.png" style="height: 40px">
              </a>
              <a href="https://www.cs.wisc.edu/" style="margin-right: 15px;">
                <img src="./static/images/uwmadison-logo.png" style="height: 60px;">
              </a>
              &nbsp;&nbsp;&nbsp;
              <a href="https://research.adobe.com/">
                <img src="./static/images/adobe-logo.png" style="height: 40px;">
              </a>
              <br>
            </div>
            <span style="font-size: 90%;">1. University of California, Los Angeles</span>
            &nbsp;&nbsp;&nbsp;
            <span style="font-size: 90%;">2. University of Wisconsin-Madison</span>
            &nbsp;&nbsp;&nbsp;
            <span style="font-size: 90%;">3. Adobe Research</span>
            <br>
            <!--              <small><i>üö©: equal advising</i></small>-->

          </div>
          <!--            <div><a href="./static/images/yochameleon-without-bg.png"><img src='./static/images/yochameleon-without-bg.png' width="200"></a></div>-->

          <div class="column has-text-centered">
            <div class="publication-links">
                <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-dark" href="ARXIV LINK"
                     target="_blank">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv (soon)</span>
                  </a>
                </span>
<!--              <span class="link-block">-->
<!--                  <a class="external-link button is-normal is-rounded is-dark" href="Github LINK"-->
<!--                     target="_blank">-->
<!--                    <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                    </span>-->
<!--                    <span>Code (Soon)</span>-->
<!--                  </a>-->
<!--                </span>-->
              <!--                 <span class="link-block">
                                <a href="https://huggingface.co/datasets/thaoshibe/YoChameleon" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                  <span class="icon">
                                    <i class="fas fa-database"></i>
                                  </span>
                                  <span>Dataset</span>
                                </a>
                              </span> -->
              <!--                 <span class="link-block">
                                <a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                  <span class="icon">
                                    <i class="fas fa-share-square"></i>
                                  </span>
                                  <span>Model</span>
                                </a>
                              </span> -->
              <!--                <span class="link-block">-->
              <!--                  <a href="POSTER" target="_blank"-->
              <!--                    class="external-link button is-normal is-rounded is-dark">-->
              <!--                    <span class="icon">-->
              <!--                      <i class="fas fa-share-square"></i>-->
              <!--                    </span>-->
              <!--                    <span>Poster (Updating)</span>-->
              <!--                  </a>-->
              <!--                </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <h4 class="subtitle has-text-centered"> -->
      <!-- <center><img src='./static/images/xfusion-teaser-2.gif' width="500"></center> -->
      <center>
        <video autoplay controls loop muted playsinline width="400">
          <source src="./static/images/xfusion-teaser.mp4" type="video/mp4">
        </video>
      </center>
      <br>
      <center>
        <text style="font-size:110%">We introduce X-Fusion - a novel framework that adapts pretrained LLMs (e.g., LLaMA)
          to new modalities (e.g., vision) <br>while retaining their language capabilities and world knowledge!
        </text>
      </center>
      <!-- </h4> -->
    </div>
  </div>
</section>

<section class="section" style="background-color:#f3f3f3">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">üìú Abstract</h2>
        <div class="content has-text-justified">
          <p>

            We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks
            while preserving their language capabilities.
            X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM‚Äôs parameters frozen
            while integrating vision-specific information for both understanding and generation.
            Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both
            image-to-text and text-to-image tasks.
            We find that incorporating understanding-focused data improves generation quality, reducing image data noise
            enhances overall performance, and feature alignment accelerates convergence for smaller models but has
            minimal impact on larger ones.
            Our findings provide valuable insights into building efficient unified multimodal models.

          </p>
        </div>
      </div>
    </div>
  </div>
</section>

</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">‚ö° X-Fusion Capabilities</h2>
        <!-- <center><img src="./static/images/xfusion-abilities.gif" width="100%"></center> -->
        <center>
          <video autoplay controls loop muted playsinline width="100%">
            <source src="./static/images/xfusion-abilities.mp4" type="video/mp4">
          </video>
        </center>

      </div>
    </div>
  </div>
</section>
<hr>

<!--  # The Start of the Main Content # -->

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">üìê X-Fusion </h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="column is-six-fifths">
          <h2 class="title is-4"> Model Architecture </h2>
          <p>
            We propose X-Fusion to address two key challenges:  (i) retaining the language abilities of the pre-trained LLM while (ii) adapting it with image generation/understanding capabilities.
            To achieve this, we introduce a <b>Dual-Tower</b> architecture that freezes all language weights and separate vision weights in each layer to help process visual information for the LLM. This approach aligns text and vision features not only at input or output level, but also at the intermediate processing level.
          </p>
        <p>
        To show the effectiveness of our approach, we proposed alternative transformer block variants for multimodal integration, including
        (a) Single Tower, (b) Gated Layer,and (c) Dual Projection. We find the Dual-Tower architecture achieves the best performance in both image generation and understanding tasks among them.
        </p>

        </div>


        <div class="content has-text-justified">
          <center>
            <td width="90%">
              <img alt="Training Pipeline" id="teaser" src="./static/images/xfusion_approach.jpg" width="98%">
            </td>
          </center>
          <div class="column is-six-fifths">
          Conceptual comparison of four model architecture baselines. Here, we illustrate how each layer processes the
          sequential multi-modal feature. (a) Single Tower: Directly fine-tuning pre-trained LLM.
          (b) Gated Layer: Duplicate Each LLM layer as the gated vision layer,
          (c) Dual Projection: Duplicate QKV matric and MLP layer for vision modality,
          (d) Dual Tower: Duplicated transformer block for vision modality.
        </div>
        </div>
      <div class="content has-text-justified">
          <div class="column is-six-fifths">
        <h2 class="title is-4"> Training Recipe</h2>
        Alongside of the model architecture, we also study the training recipe for X-Fusion. We focused on two key questions:
            <ol>
              <li> How does noise level affect performance in a joint training setting? </li>
              <li> Does multitask training provide mutual benefits between tasks? </li>
            </ol>

        We first study the effect of noise level in the training data.
        While diffusion-based image modeling benefits from noisy input for generation tasks,
        excessive noise can degrade visual quality and hinder image understanding.
        In contrast to conventional choose maximum of \( t = 50\% T \) to reduce distortion for visual understanding,
           we find  <b>clean images for visual understanding improve performance for both tasks</b>.
        We show the performance comparison between varied the max noise level for image understanding (I2T) tasks:
        0% (clean image), 25%, 50%, 75%, and 100% (normal generation setting) in the following figure.
        </div>
      </div>

        <div>

          <center><img src="./static/images/noise.png" width="100%"></img></center>
        </div>
        <br>

              <div class="content has-text-justified">
          <div class="column is-six-fifths">
<!--        <h2 class="title is-4"> Training Recipe</h2>-->
        We then study the effect of data ratio in the training data. To investigate the synergy
        between visual understanding and generation from data's perspective, we begin with the training data composed
        entirely of T2I tasks (100% T2I, 0% I2T, or denoted as 100/0 for short), then progressively decrease the proportion
        of T2I data while increasing I2T data (i.e., 66/33, 50/50, 33/66) until the dataset consists solely of I2T tasks (0/100).
        We find <b>the asymmetric relationship: understanding data benefits generation, but generation data does not enhance understanding. </b>
        </div>
      </div>

        <div>
          <center><img src="./static/images/data-ratio.png" width="100%"></img></center>
        </div>
        <br>
        <center><b>‚ùó Please refer to the main paper for detailed ablation experiments! ‚ùó</b></center>
      </div>
    </div>
  </div>
  <hr>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">üñºÔ∏è Experiment Result</h2>
              <div class="content has-text-justified">
        <div class="column is-six-fifths">
      <h2 class="title is-3">         Text-to-Image Generation</h2>

                  <center><img src="./static/images/xfusion_results_gen.jpg" width="90% px"></center>
      </div>
                </div>



        <hr>
        <br><br>

              <div class="content has-text-justified">
        <div class="column is-six-fifths">
      <h2 class="title is-3">         Text-to-Image Generation</h2>

                  <center><img src="./static/images/xfusion_qual_2.jpg" width="90% px"></center>
      </div>
                </div>
<!--&ndash;&gt;-->
        <hr>
        <br><br>
                <div class="content has-text-justified">
          <div class="column is-six-fifths">
        <h2 class="title is-3"> Fine-tuned X-Fusion on Downsteam Tasks</h2>
        Can X-Fusion extend its capabilities to other downstream vision-and-language tasks?
        We fine-tuned our model on four tasks simultaneously‚Äîincluding image editing, localization,
        in/oupt-painting, and Visual Question Answering using internal datasets for 20k training steps.
        In the following figure, we demonstrate that our unified X-Fusion model can handle multiple tasks without
        creating task-specific weights.
        </div>
          </div>

        <center><img src="./static/images/xfusion_four_tasks_web.png" width="98% px"></center>
               <br><br>
        <hr>
        <br><br>

<!--        <br><br>-->
        <div class="content has-text-justified">
          <div class="column is-six-fifths">
        <h2 class="title is-3"> Transfer from Pretrained Diffusion Model</h2>

        One main advantage of our dual-tower design is that allows non-identical block designs in language and vision towers as each block in the both tower processes the entire feature sequence independently.
            With this advantage, we could transfer the image generation capability from a large-scale pretrained diffusion model that uses diffusion transformers.
            We trained a a variation of X-Fusion using Llama3 model as the language tower and an in-house pretrained text-to-image DiT model as its vision tower, notated as X-Fusion(Pretrained DiT), with 50K steps.
            We show that X-Fusion(Pretrained DiT) obtained stronger image generation capability compared to the vanilla X-Fusion.

        </div>
          </div>
        <center><img src="./static/images/xfusion-with-dit-web_v1.svg" width="96% px"></center>
        <!--          Our X-Fusion model can also be used with pre-trained DiT model, which is a diffusion-based text-to-image generation model.-->
        <!--          <hr>-->
        <!--          <br><br>-->
        <!--          <h2 class="title is-3">üñºÔ∏è Qualitative Result: Multi-Turn Generation</h2>-->
        <!--          <center><img src="./static/images/xfusion_qual_4_conv_gen.jpg" width="90% px"></center>-->

      </div>
    </div>
  </div>
</section>
<hr>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">üì• BibTeX</h2>
    <pre><code>
        to be updated
  </code></pre>
  </div>

  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">üíå Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/"
                                                                                             rel="license">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <center>
        <i>Thank you (.‚ùõ ·¥ó ‚ùõ.).</i>
        <p>
        </p>
    </div>
  </section>
</body>

</html>
